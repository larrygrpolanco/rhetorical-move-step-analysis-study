[1a] Reinforcement learning (RL) algorithms hypothesize that, in value-based decision-making tasks, animals maintain an action value map and update it using reward prediction errors (RPE), the difference between the observed and predicted reward associated with the chosen action.
[1b] The growing family of RL models has proved remarkably successful for explaining trial-by-trial changes in behavior and the responses of dopamine neurons in rodents and primates.
[2b] However, these standard RL (SRL) models may not account as well for behavioral and neural data when sensory observations are not sufficient to determine optimal behavior, for example, when the underlying reward distribution changes over time due to the existence of a hidden state.
[2c] To solve such tasks, evidence suggests that the brain represents a “belief state” (a probability distribution over hidden states), updated by Bayesian inference, or possibly learned implicitly in an end-to-end fashion.
[2b][2c] The significance of Bayesian inference becomes apparent when studying mouse behavior in a two-armed bandit task (2ABT, Fig. 1A), a serial reversal learning task where rewards for correct choices are delivered on a probabilistic schedule, and the correct port switches between two available options across blocks of trials within a single session.
[2c] Both mice and human subjects trained in this task are capable of maintaining stable behavior within a block, and then rapidly adapting after reward contingencies reverse following a block switch.
[2b] The simplest versions of standard RL models do not capture choice stability within a block and rapid switching when the rewarded port changes.
[2b] Bayesian inference may offer better models of behavior in tasks like the 2ABT because they capture stable within-block choice and rapid between-block switching.
[2c] However, the additional explanatory power of Bayesian models with respect to RL models is currently ambiguous.
[2c] Comparison of Bayesian and RL models using human behavior in the 2ABT suggested that they both provide complementary explanations of behavior in the 2ABT (provided the RL models are relatively sophisticated, as explained below), but neither was decisively superior.
[2c] The limitations of a purely Bayesian account of reversal learning were also challenged by recent behavioral analyses of mice performing the 2ABT.
[2c] In addition to comparing “pure” Bayesian and SRL models, it is also valuable to include a hybrid model, as past studies suggest that Bayesian and RL processes may be combined in the brain.
[2c] We hypothesized that RPE signals generated by the BRL model would match dopamine measurements more closely than RPE signals generated by alternative models.
[2c] We also hypothesized that the BRL model would better match the choice data, in particular the rapid adaptation to block switches exhibited by mice.